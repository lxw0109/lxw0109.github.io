<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="References 机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题 详解梯度爆炸和梯度消失 详解机器学习中的梯度消失、爆炸原因及其解决方法 文中存在一些错误，本文对其进行了修正 使用 Markdown + MathJax 在博客里插入数学公式 把公式转换为图片 F(x)=x^2 hexo博客配置MathJax How t">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度消失与梯度爆炸">
<meta property="og:url" content="http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/index.html">
<meta property="og:site_name" content="Xiaowei Liu&#39;s Blog">
<meta property="og:description" content="References 机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题 详解梯度爆炸和梯度消失 详解机器学习中的梯度消失、爆炸原因及其解决方法 文中存在一些错误，本文对其进行了修正 使用 Markdown + MathJax 在博客里插入数学公式 把公式转换为图片 F(x)=x^2 hexo博客配置MathJax How t">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/sigmoid_sigmoid_derivative.png">
<meta property="og:image" content="http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/sigmoid_vs_tanh.png">
<meta property="og:image" content="http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/sigmoid_vs_tanh_vs_relu.png">
<meta property="og:updated_time" content="2018-08-19T07:59:34.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梯度消失与梯度爆炸">
<meta name="twitter:description" content="References 机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题 详解梯度爆炸和梯度消失 详解机器学习中的梯度消失、爆炸原因及其解决方法 文中存在一些错误，本文对其进行了修正 使用 Markdown + MathJax 在博客里插入数学公式 把公式转换为图片 F(x)=x^2 hexo博客配置MathJax How t">
<meta name="twitter:image" content="http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/sigmoid_sigmoid_derivative.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="shortcut icon" href="/images/favicon.ico">
          
        
    
    <!-- title -->
    <title>梯度消失与梯度爆炸</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

<body>
    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/04/17/GIL-in-Python-CPython/"><i class="fa fa-chevron-left" aria-hidden="true" onmouseover='$("#i-prev").toggle();' onmouseout='$("#i-prev").toggle();'></i></a></li>
        
        
        <li><a class="icon" href="/2018/03/20/Basic-Mathematics-for-Machine-Learning/"><i class="fa fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle();' onmouseout='$("#i-next").toggle();'></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle();' onmouseout='$("#i-top").toggle();'></i></a></li>
        <li><a class="icon" href="#"><i class="fa fa-share-alt" aria-hidden="true" onmouseover='$("#i-share").toggle();' onmouseout='$("#i-share").toggle();' onclick='$("#share").toggle();return false;'></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/"><i class="fa fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&text=梯度消失与梯度爆炸"><i class="fa fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&is_video=false&description=梯度消失与梯度爆炸"><i class="fa fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=梯度消失与梯度爆炸&body=Check out this article: http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/"><i class="fa fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&name=梯度消失与梯度爆炸&description="><i class="fa fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.</span> <span class="toc-text">References</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-梯度不稳定问题"><span class="toc-number">2.</span> <span class="toc-text">1. 梯度不稳定问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-梯度消失-amp-梯度爆炸"><span class="toc-number">3.</span> <span class="toc-text">2. 梯度消失 &amp; 梯度爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-深层网络角度"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 深层网络角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-激活函数角度"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 激活函数角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-几个问题"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 几个问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-如何解决梯度消失和梯度爆炸？"><span class="toc-number">4.</span> <span class="toc-text">3. 如何解决梯度消失和梯度爆炸？</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index width mx-auto px2 my4">
        <!-- -->
        <header id="header">
  <a href="/">
  
    
      <div id="logo" style="background-image: url(/images/lxw.png);"></div>
    
  
    <div id="title">
      <h1>Xiaowei Liu&#39;s Blog</h1>
    </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#"><i class="fa fa-bars fa-2x"></i></a>
      </li>
       
        <li><a href="/">Home</a></li>
       
        <li><a href="/archives/">Articles</a></li>
       
        <li><a href="/about/">About</a></li>
      
    </ul>
  </div>
</header>

        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        梯度消失与梯度爆炸
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Xiaowei Liu's Blog</span>
      </span>
      
    <div class="postdate">
        <time datetime="2018-03-20T10:29:12.000Z" itemprop="datePublished">2018-03-20</time>
    </div>


          <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>
    </div>

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="http://blog.csdn.net/cppjava_/article/details/68941436" target="_blank" rel="noopener">机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题</a></li>
<li><a href="https://www.cnblogs.com/DLlearning/p/8177273.html" target="_blank" rel="noopener">详解梯度爆炸和梯度消失</a></li>
<li><a href="http://blog.csdn.net/qq_25737169/article/details/78847691" target="_blank" rel="noopener">详解机器学习中的梯度消失、爆炸原因及其解决方法</a> 文中存在一些错误，本文对其进行了修正</li>
<li><a href="http://blog.csdn.net/kamidox/article/details/48380239" target="_blank" rel="noopener">使用 Markdown + MathJax 在博客里插入数学公式</a></li>
<li>把公式转换为图片 <a href="http://latex.codecogs.com/gif.latex?f=x^2" target="_blank" rel="noopener">F(x)=x^2</a></li>
<li>hexo博客配置MathJax <a href="https://github.com/hexojs/hexo-math/issues/26" target="_blank" rel="noopener">How to config it to make it work?</a></li>
</ol>
<h2 id="1-梯度不稳定问题"><a href="#1-梯度不稳定问题" class="headerlink" title="1. 梯度不稳定问题"></a>1. 梯度不稳定问题</h2><p>什么是梯度不稳定问题?<br>深度神经网络中的梯度不稳定性，前面层中的梯度或会消失，或会爆炸。<br><strong>原因</strong>: 前面层上的梯度是来自于后面层上梯度的乘积(后面的公式)。当存在过多的层次时，就出现了内在本质上的不稳定场景，如梯度消失和梯度爆炸。<br>目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。<br><strong>整个深度网络可以视为是一个复合的非线性多元函数</strong>：</p>
<script type="math/tex; mode=display">F(x)=f_n(...f_3(f_2(f_1(x)∗θ_1+b_1)∗θ_2+b_2)...)</script><h2 id="2-梯度消失-amp-梯度爆炸"><a href="#2-梯度消失-amp-梯度爆炸" class="headerlink" title="2. 梯度消失 &amp; 梯度爆炸"></a>2. 梯度消失 &amp; 梯度爆炸</h2><p><strong>梯度消失(vanishing gradient problem)</strong>与<strong>梯度爆炸(exploding gradient problem)</strong>其实是一种情况。其中，<strong>梯度消失</strong>经常出现，一是在<strong>深层网络</strong>中，二是采用了<strong>不合适的激活函数</strong>(比如sigmoid)。<strong>梯度爆炸</strong>一般出现在<strong>深层网络</strong>和<strong>权值初始化值太大</strong>的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。</p>
<h3 id="2-1-深层网络角度"><a href="#2-1-深层网络角度" class="headerlink" title="2.1 深层网络角度"></a>2.1 深层网络角度</h3><p>假设每一层网络激活后的输出为 $f<em>i(x)$ ,其中i为第i层, x代表第i层的输入，也就是第i−1层的输出，f是激活函数，那么，得出 $f</em>{i+1}=f(f<em>{i}∗w</em>{i+1}+b<em>{i+1})$ ,简单记为 $f</em>{i+1}=f(f<em>{i}∗w</em>{i+1})$ 。<br>BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为w←w+Δw，给定学习率 $\alpha$ ，得出<br>$\Delta{w}=−\alpha\frac{\partial{Loss}}{\partial{w}}$ 。如果要更新第二隐藏层的权值信息，根据链式求导法则，更新梯度信息： </p>
<script type="math/tex; mode=display">\Delta w_1=\frac{\partial{Loss}}{\partial{w_2}}=\frac{\partial{Loss}}{\partial{f_4}}\frac{\partial{f_4}}{\partial{f_3}}\frac{\partial{f_3}}{\partial{f_2}}\frac{\partial{f_2}}{\partial{w_2}}</script><p>很容易看出来 $\frac{\partial{f_2}}{\partial{w_2}}=f_1$ 即第二隐藏层的输入。<br>所以说， $\frac{\partial{f_4}}{\partial{f_3}}$ 就是对激活函数进行求导，<strong>如果此部分大于1</strong>，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生<strong>梯度爆炸</strong>，<strong>如果此部分小于1</strong>，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了<strong>梯度消失</strong>。如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题。<br><strong>总结</strong>：从深层网络角度来讲，不同的层学习的速度差异很大，表现为<strong>网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢</strong>，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。<strong>因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足</strong>，另外多说一句，Hinton提出capsule的原因就是为了彻底抛弃反向传播，如果真能大范围普及，那真是一个革命。<br>对于Sigmoid激活函数，前面的层(离输出层远的层)比后面的层(离输出层近的层)梯度变化更小，故变化更慢，从而引起了梯度消失问题。当权值过大，前面层比后面层梯度变化更快，会引起梯度爆炸问题。</p>
<h3 id="2-2-激活函数角度"><a href="#2-2-激活函数角度" class="headerlink" title="2.2 激活函数角度"></a>2.2 激活函数角度</h3><p>上文中提到计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid激活函数图，右边是其导数的图像，如果使用sigmoid作为激活函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。<br>$sigmoid(x) = \frac{1}{1+e^{-x}} $ &emsp; $sigmoid’(x) = \frac{e^{-x}}{1+e^{-x}}$<br>sigmoid函数及其导函数的曲线如下图所示:<br><img src="./sigmoid_sigmoid_derivative.png" alt=""><br>同理，tanh作为激活函数，其导数图如下，可以看出，<strong>tanh比sigmoid要好一些，但是它的导数仍然是小于1的</strong>。<br>$ tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $ &emsp; $ tanh’(x) = 1 - \frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2} = \frac{4}{(e^x+e^{-x})^2} $<br>tanh函数及其导函数的曲线如下图所示(与sigmoid函数的对比):<br><img src="./sigmoid_vs_tanh.png" alt="">  </p>
<h3 id="2-3-几个问题"><a href="#2-3-几个问题" class="headerlink" title="2.3 几个问题"></a>2.3 几个问题</h3><p><strong>1.当采用Sigmoid激活函数时，梯度消失和梯度爆炸哪个更易发生？</strong><br>分析梯度爆炸出现时a的取值范围：因为Sigmoid导数最大为1/4，故只有当abs(w)&gt;4时才可能出现 $abs(w_j * \sigma’(z_j))&gt;1$ 。由此计算出a的数值变化范围很小，仅仅在此窄范围内会出现梯度爆炸问题。而最普遍发生的是梯度消失问题。</p>
<p><strong>2.如何确定是否出现梯度爆炸？</strong><br>训练过程中出现梯度爆炸会伴随一些细微的信号，如：</p>
<ul>
<li>模型无法从训练数据中获得更新（如低损失）</li>
<li>模型不稳定，导致更新过程中的损失出现显著变化</li>
<li>训练过程中，模型损失变成 NaN</li>
</ul>
<h2 id="3-如何解决梯度消失和梯度爆炸？"><a href="#3-如何解决梯度消失和梯度爆炸？" class="headerlink" title="3. 如何解决梯度消失和梯度爆炸？"></a>3. 如何解决梯度消失和梯度爆炸？</h2><ul>
<li>重新设计网络结构<ul>
<li>在深度神经网络中，梯度爆炸可以通过<strong>重新设计层数更少的网络</strong>来解决。</li>
<li><strong>使用更小的批尺寸</strong>对网络训练也有好处。另外也许是<strong>学习率的原因，学习率过大导致的问题，减小学习率</strong>。</li>
<li>在<strong>循环神经网络中，训练过程中在更少的先前时间步上进行更新</strong>（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。</li>
</ul>
</li>
<li>预训练加微调<br>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其<strong>基本思想是每次训练一层隐节点</strong>，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是<strong>逐层“预训练”（pre-training）</strong>；<strong>在预训练完成后，再对整个网络进行“微调”（fine-tunning）</strong>。Hinton在训练深度信念网络(Deep Belief Networks)中，使用了这个方法，<strong>在各层预训练完成后，再利用BP算法对整个网络进行训练</strong>。<strong>此思想相当于是先寻找局部最优，然后整合起来寻找全局最优</strong>，<strong>此方法有一定的好处，但是目前应用的不是很多了</strong>。</li>
<li>梯度剪切(Gradient Clipping)、正则<br>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。<br>另外一种解决梯度爆炸的手段是采用<strong>权重正则化（weithts regularization）</strong>比较常见的是l1正则(权重绝对值)和l2正则(权重平方)，在各个深度框架中都有相应的API可以使用正则化。<br>正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：  <script type="math/tex; mode=display">Loss=(y−W^{T}x)^2+\alpha||W||_2</script>其中， $\alpha$ 是指正则项系数，<strong>如果发生梯度爆炸，权值的 范数 就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生</strong>。</li>
<li><p>使用ReLU,maxout等替代sigmoid<br>ReLU的思想很简单，如果激活函数的导数为1，那么就不存在梯度消失梯度爆炸的问题了，每层的网络都可以得到相同的更新速度。  </p>
<script type="math/tex; mode=display">ReLU(x) = max(0, x) = \begin{cases} 0, & x<0 \\ x, & x \ge 0 \end{cases}</script><p>sigmoid函数的梯度随着x的增大或减小而消失，但ReLU不会。<br>ReLU函数及其导函数的曲线如下图所示(与sigmoid/tanh函数的对比):<br><img src="./sigmoid_vs_tanh_vs_relu.png" alt=""><br>从上图可以看出，ReLU函数的导数在正数部分是恒等于1的，因此在深层网络中使用ReLU激活函数就不会导致梯度消失和爆炸的问题。<br>ReLU的主要贡献在于：  </p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时ReLU也存在一些缺点：  </p>
<ul>
<li>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</li>
<li>输出不是以0为中心的</li>
</ul>
<p>尽管ReLU也有缺点，但是仍然是目前使用最多的激活函数。<strong>采用ReLU激活函数是最适合隐藏层的新实践</strong>。</p>
</li>
<li>LSTM<br>LSTM全称是<strong>长短期记忆网络（long-short term memory networks）</strong>，是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可以在更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。目前也有基于CNN的LSTM。<strong>采用LSTM单元是适合循环神经网络的序列预测的最新最好实践</strong>。</li>
<li>Batchnorm<br>Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有<strong>加速网络收敛速度，提升训练稳定性的效果</strong>，<strong>Batchnorm本质上是解决反向传播过程中的梯度问题</strong>。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号规范化到均值为0，方差为1保证网络的稳定性。 </li>
<li>残差结构</li>
</ul>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.</span> <span class="toc-text">References</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-梯度不稳定问题"><span class="toc-number">2.</span> <span class="toc-text">1. 梯度不稳定问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-梯度消失-amp-梯度爆炸"><span class="toc-number">3.</span> <span class="toc-text">2. 梯度消失 &amp; 梯度爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-深层网络角度"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 深层网络角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-激活函数角度"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 激活函数角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-几个问题"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 几个问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-如何解决梯度消失和梯度爆炸？"><span class="toc-number">4.</span> <span class="toc-text">3. 如何解决梯度消失和梯度爆炸？</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/"><i class="fa fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&text=梯度消失与梯度爆炸"><i class="fa fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&is_video=false&description=梯度消失与梯度爆炸"><i class="fa fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=梯度消失与梯度爆炸&body=Check out this article: http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&title=梯度消失与梯度爆炸"><i class="fa fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://xiaoweiliu.cn/2018/03/20/梯度消失与梯度爆炸/&name=梯度消失与梯度爆炸&description="><i class="fa fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick='$("#toc-footer").toggle();return false;'><i class="fa fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick='$("#share-footer").toggle();return false;'><i class="fa fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick='$("#nav-footer").toggle();return false;'><i class="fa fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Xiaowei Liu
  </div>
  <!--
  <div class="footer-right">
    <nav>
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/archives/">Articles</a></li>
        
          <li><a href="/about/">About</a></li>
        
      </ul>
    </nav>
  </div>
  -->
</footer>

    <script type="text/javascript" color="43,188,138" opacity="0.5" zindex="-2" count="36" src="js/canvas-nest.js"></script>    <!-- 背景蒲公英 -->
    <!-- <embed width="200" height="200" align="middle" src="http://images.cnblogs.com/cnblogs_com/csharp/clock.swf" quality="high" pluginspage="http://www.macromedia.com/go/getflashplayer" type="application/x-shockwave-flash"> --> <!-- clock -->
    <script>
    (function() {
        var OriginTitile = document.title, titleTime;
        document.addEventListener('visibilitychange', function() {
            if (document.hidden) {
                document.title = 'Will you be back?';
                clearTimeout(titleTime);
            } else {
                document.title = 'Welcome back!';
                titleTime = setTimeout(function() {
                    document.title = OriginTitile;
                },2000);
            }
        });
    })();
    </script>

    <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
            inlineMath: [ ['$$', '$$'] ],
            displayMath: [ ['$$$', '$$$']],
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        messageStyle: "none",
        "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
<link rel="stylesheet" href="/lib/meslo-LG/styles.css">
<link rel="stylesheet" href="/lib/justified-gallery/justifiedGallery.min.css">


<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- Google Analytics -->

<!-- Disqus Comments -->


